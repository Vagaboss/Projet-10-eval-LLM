import streamlit as st
import os
import logging
import time
import logfire
from mistralai.client import MistralClient
from mistralai.models.chat_completion import ChatMessage
from dotenv import load_dotenv

# --- Importations internes ---
try:
    from utils.config import (
        MISTRAL_API_KEY, MODEL_NAME, SEARCH_K,
        APP_TITLE, NAME
    )
    from utils.vector_store import VectorStoreManager
except ImportError as e:
    st.error(f"Erreur d'importation: {e}. V√©rifiez la structure de vos dossiers et les fichiers dans 'utils'.")
    st.stop()

# --- Configuration des logs ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Initialisation Logfire ---
logfire.configure()
logfire.info("üöÄ D√©marrage de l‚Äôapplication MistralChat (RAG)")

# --- Configuration de l‚ÄôAPI Mistral ---
api_key = MISTRAL_API_KEY
model = MODEL_NAME

if not api_key:
    st.error("Erreur : Cl√© API Mistral non trouv√©e dans le .env.")
    st.stop()

try:
    client = MistralClient(api_key=api_key)
    logfire.info("Client Mistral initialis√© avec succ√®s", extra={"model": model})
except Exception as e:
    st.error(f"Erreur lors de l‚Äôinitialisation du client Mistral : {e}")
    logfire.error("Erreur client Mistral", extra={"exception": str(e)})
    st.stop()

# --- Chargement du Vector Store ---
@st.cache_resource
def get_vector_store_manager():
    with logfire.span("Chargement du VectorStoreManager"):
        manager = VectorStoreManager()
        if manager.index is None or not manager.document_chunks:
            logfire.warning("VectorStoreManager vide ou non initialis√©")
            st.warning("L‚Äôindex FAISS est vide. Lancez `python indexer.py` avant.")
            return None
        logfire.info("VectorStoreManager charg√©", extra={"vecteurs": manager.index.ntotal})
        return manager

vector_store_manager = get_vector_store_manager()

# --- Prompt syst√®me RAG ---
SYSTEM_PROMPT = f"""Tu es 'NBA Analyst AI', un assistant expert sur la ligue NBA.
R√©ponds de fa√ßon claire et analytique aux questions des fans.

---
{{context_str}}
---

QUESTION DU FAN :
{{question}}

R√âPONSE DE L'ANALYSTE NBA :
"""

# --- Initialisation de la session ---
if "messages" not in st.session_state:
    st.session_state.messages = [{
        "role": "assistant",
        "content": f"üëã Bonjour ! Je suis votre analyste IA pour la {NAME}. Posez-moi vos questions sur les √©quipes, les joueurs ou les stats !"
    }]

# --- Fonction de g√©n√©ration de r√©ponse ---
@logfire.instrument("G√©n√©ration de r√©ponse via Mistral")
def generer_reponse(prompt_messages: list[ChatMessage]) -> str:
    """Envoie le prompt √† l‚ÄôAPI Mistral et retourne la r√©ponse."""
    if not prompt_messages:
        logfire.warning("Appel √† generer_reponse avec prompt vide")
        return "Je ne peux pas traiter une demande vide."

    try:
        with logfire.span("Appel API Mistral"):
            start_time = time.time()
            response = client.chat(
                model=model,
                messages=prompt_messages,
                temperature=0.1,
            )
            elapsed = round(time.time() - start_time, 2)

        if response.choices and len(response.choices) > 0:
            content = response.choices[0].message.content
            logfire.info("R√©ponse Mistral g√©n√©r√©e", extra={
                "temps_reponse_s": elapsed,
                "longueur": len(content)
            })
            return content
        else:
            logfire.warning("R√©ponse Mistral vide ou invalide")
            return "D√©sol√©, je n‚Äôai pas pu g√©n√©rer de r√©ponse valide."

    except Exception as e:
        logfire.error("Erreur pendant l‚Äôappel API Mistral", extra={"exception": str(e)})
        return f"Erreur Mistral : {e}"

# --- Interface Streamlit ---
st.title(APP_TITLE)
st.caption(f"Assistant virtuel NBA | Mod√®le : {model}")

# --- Historique de chat ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# --- Saisie utilisateur ---
if prompt := st.chat_input(f"Posez votre question sur la {NAME}..."):
    with logfire.span("Nouvelle question utilisateur", extra={"question": prompt}):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)

        # --- √âtape 1 : Recherche FAISS ---
        with logfire.span("Recherche de contexte dans Vector Store"):
            if vector_store_manager is None:
                logfire.error("VectorStoreManager non disponible")
                st.error("Index FAISS non disponible.")
                st.stop()

            start_time = time.time()
            search_results = vector_store_manager.search(prompt, k=SEARCH_K)
            elapsed = round(time.time() - start_time, 2)
            logfire.info("Recherche FAISS termin√©e", extra={
                "nb_chunks": len(search_results),
                "duree_s": elapsed
            })

        # --- √âtape 2 : Pr√©paration du contexte ---
        if search_results:
            context_str = "\n\n---\n\n".join([
                f"Source: {res['metadata'].get('source', 'Inconnue')} (Score: {res['score']:.1f}%)\nContenu: {res['text']}"
                for res in search_results
            ])
            logfire.info("Contexte trouv√©", extra={"nb_chunks": len(search_results)})
        else:
            context_str = "Aucune information pertinente trouv√©e."
            logfire.warning("Aucun contexte pertinent trouv√© pour la question")

        # --- √âtape 3 : G√©n√©ration de la r√©ponse ---
        with logfire.span("Appel Mistral avec contexte"):
            final_prompt = SYSTEM_PROMPT.format(context_str=context_str, question=prompt)
            messages_for_api = [ChatMessage(role="user", content=final_prompt)]
            response_content = generer_reponse(messages_for_api)

        # --- √âtape 4 : Affichage et historique ---
        with st.chat_message("assistant"):
            st.write(response_content)
        st.session_state.messages.append({"role": "assistant", "content": response_content})

        logfire.info("R√©ponse affich√©e", extra={
            "question": prompt,
            "r√©ponse_partielle": response_content[:150]
        })

# --- Pied de page ---
st.markdown("---")
st.caption("‚öôÔ∏è Powered by Mistral AI & FAISS | Trac√© en direct avec Pydantic Logfire")
